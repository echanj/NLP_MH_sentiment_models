{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6b4f3e0-57f5-4563-aea0-ba8a7e905b4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "# Cause plots to be displayed in the notebook:\n",
    "%pylab inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e18a409-5fe3-4862-bf71-3d4ff5899f29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 10:19:55.870624: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import regex as re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eed4af8c-3062-49e2-b883-8c2c5f457123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Create a corpus from a list of texts\n",
    "common_dictionary = Dictionary(common_texts)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in common_texts]\n",
    "\n",
    "# Train the model on the corpus.\n",
    "lda = LdaModel(common_corpus, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "053342b0-c3e9-42dc-b958-a345a62aead3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.ldamodel.LdaModel at 0x165ae2450>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd65cb0-80fe-4796-b0d3-3e448a2b557b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc74a810-0c34-4763-b224-e55581c41892",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "D1 = 'I want to watch a movie this weekend.'\n",
    "D2 =  'I went shopping yesterday. New Zealand won the World Test Championship by beating India by eight wickets at Southampton.'\n",
    "D3 =  'I don’t watch cricket. Netflix and Amazon Prime have very good movies to watch.'\n",
    "D4 =  'Movies are a nice way to chill however, this time I would like to paint and read some good books. It’s been long!'\n",
    "D5 =  'This blueberry milkshake is so good! Try reading Dr. Joe Dispenza’s books. His work is such a game-changer! His books helped to learn so much about how our thoughts impact our biology and how we can all rewire our brains.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41568b2f-34bb-4676-8eb7-b16db3a6fd89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# combining all the documents into a list:\n",
    "\n",
    "corpus = [D1, D2, D3, D4, D5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d4ec97a-8c4a-4ee4-aeb7-b3e272da5a32",
   "metadata": {
    "executionInfo": {
     "elapsed": 6177,
     "status": "ok",
     "timestamp": 1603266387620,
     "user": {
      "displayName": "Aishwarya Singh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgeJwfn4BdBDCAplWi_kdtB9FRssOpXO7T_aMgg=s64",
      "userId": "01105858832371513140"
     },
     "user_tz": -330
    },
    "id": "fNATC-NcaVrt",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I want to watch a movie this weekend.',\n",
       " 'I went shopping yesterday. New Zealand won the World Test Championship by beating India by eight wickets at Southampton.',\n",
       " 'I don’t watch cricket. Netflix and Amazon Prime have very good movies to watch.',\n",
       " 'Movies are a nice way to chill however, this time I would like to paint and read some good books. It’s been long!',\n",
       " 'This blueberry milkshake is so good! Try reading Dr. Joe Dispenza’s books. His work is such a game-changer! His books helped to learn so much about how our thoughts impact our biology and how we can all rewire our brains.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the complete corpus as below:\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f67df3cf-9813-4fab-9fe1-732ea308027a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply Preprocessing on the Corpus using nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# stop loss words \n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "# punctuation \n",
    "exclude = set(string.punctuation) \n",
    "\n",
    "# lemmatization\n",
    "lemma = WordNetLemmatizer() \n",
    "\n",
    "# One function for all the steps:\n",
    "def clean(doc):\n",
    "    \n",
    "    # convert text into lower case + split into words\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    \n",
    "    # remove any stop words present\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)  \n",
    "    \n",
    "    # remove punctuations + normalize the text\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())  \n",
    "    return normalized\n",
    "\n",
    "# clean data stored in a new list\n",
    "clean_corpus = [clean(doc).split() for doc in corpus]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b11b2991-3589-4da6-bc1c-1dd95609a269",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['want', 'watch', 'movie', 'weekend'],\n",
       " ['went',\n",
       "  'shopping',\n",
       "  'yesterday',\n",
       "  'new',\n",
       "  'zealand',\n",
       "  'world',\n",
       "  'test',\n",
       "  'championship',\n",
       "  'beating',\n",
       "  'india',\n",
       "  'eight',\n",
       "  'wicket',\n",
       "  'southampton'],\n",
       " ['don’t',\n",
       "  'watch',\n",
       "  'cricket',\n",
       "  'netflix',\n",
       "  'amazon',\n",
       "  'prime',\n",
       "  'good',\n",
       "  'movie',\n",
       "  'watch'],\n",
       " ['movie',\n",
       "  'nice',\n",
       "  'way',\n",
       "  'chill',\n",
       "  'however',\n",
       "  'time',\n",
       "  'would',\n",
       "  'like',\n",
       "  'paint',\n",
       "  'read',\n",
       "  'good',\n",
       "  'book',\n",
       "  'it’s',\n",
       "  'long'],\n",
       " ['blueberry',\n",
       "  'milkshake',\n",
       "  'good',\n",
       "  'try',\n",
       "  'reading',\n",
       "  'dr',\n",
       "  'joe',\n",
       "  'dispenza’s',\n",
       "  'book',\n",
       "  'work',\n",
       "  'gamechanger',\n",
       "  'book',\n",
       "  'helped',\n",
       "  'learn',\n",
       "  'much',\n",
       "  'thought',\n",
       "  'impact',\n",
       "  'biology',\n",
       "  'rewire',\n",
       "  'brain']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d62cd6a-8289-4dd4-8b18-1cb8b23d8f0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2d71f3e-be9e-4450-b186-6850fee80a25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:32:58.911623Z",
     "start_time": "2019-06-17T01:32:58.897659Z"
    },
    "id": "sh_uDWcCLcgI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_text(text):\n",
    "    '''\n",
    "    Use techniques learned in previous labs. Remove StopWords, Punctuation, Lemmatize, lowercase etc.\n",
    "    '''\n",
    "    doc = nlp(text)\n",
    "    # convert all tokens to lowercase\n",
    "    # doc_lower = [token.lower_ for token in doc]\n",
    "  \n",
    "    # Removes StopWords, Punctuation and Lemmatize inline \n",
    "    text=' '.join(['' if (t.is_stop |  t.is_punct )else t.lemma_.lower() for t in doc]) \n",
    "    \n",
    "    text = re.sub(r'\\n', ' ', text)  # remove newline\n",
    "    text = re.sub(r'\\s+', ' ', text)  # clean up spacing\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe03481b-94fb-4636-b2e4-04e02fd074a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['want', 'watch', 'movie', 'weekend'],\n",
       " ['go',\n",
       "  'shop',\n",
       "  'yesterday',\n",
       "  'new',\n",
       "  'zealand',\n",
       "  'win',\n",
       "  'world',\n",
       "  'test',\n",
       "  'championship',\n",
       "  'beat',\n",
       "  'india',\n",
       "  'wicket',\n",
       "  'southampton'],\n",
       " ['watch', 'cricket', 'netflix', 'amazon', 'prime', 'good', 'movie', 'watch'],\n",
       " ['movie',\n",
       "  'nice',\n",
       "  'way',\n",
       "  'chill',\n",
       "  'time',\n",
       "  'like',\n",
       "  'paint',\n",
       "  'read',\n",
       "  'good',\n",
       "  'book',\n",
       "  'long'],\n",
       " ['blueberry',\n",
       "  'milkshake',\n",
       "  'good',\n",
       "  'try',\n",
       "  'read',\n",
       "  'dr.',\n",
       "  'joe',\n",
       "  'dispenza',\n",
       "  'book',\n",
       "  'work',\n",
       "  'game',\n",
       "  'changer',\n",
       "  'book',\n",
       "  'help',\n",
       "  'learn',\n",
       "  'thought',\n",
       "  'impact',\n",
       "  'biology',\n",
       "  'rewire',\n",
       "  'brain']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctext=[convert_text(doc) for doc in corpus]\n",
    "[doc.split() for doc in ctext] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a522f3-ae67-4009-8af4-5338260d8a87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
